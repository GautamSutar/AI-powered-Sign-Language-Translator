{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "212ccc5a-a36a-497e-9cfb-46bb6860f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25369462-35ab-411e-bc7f-8894ebeea9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e7254-f90f-438a-be3d-9b36d012b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_vector(results):\n",
    "    lh = np.zeros(21*3)\n",
    "    rh = np.zeros(21*3)\n",
    "    if results.multi_hand_landmarks and results.multi_handedness:\n",
    "        for hand_index, hand_info in enumerate(results.multi_handedness):\n",
    "            hand_label = hand_info.classification[0].label\n",
    "            hand_landmarks = results.multi_hand_landmarks[hand_index]\n",
    "            if hand_label == \"Left\":\n",
    "                lh = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten()\n",
    "            elif hand_label == \"Right\":\n",
    "                rh = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten()     \n",
    "    return np.concatenate([lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009454d7-cf30-4525-b4ff-b4eace5335cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_hand_landmarks and results.multi_handedness:\n",
    "            for hand_index, hand_info in enumerate(results.multi_handedness):\n",
    "                hand_label = hand_info.classification[0].label\n",
    "                hand_landmarks = results.multi_hand_landmarks[hand_index]\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS)\n",
    "                wrist = hand_landmarks.landmark[0]\n",
    "                h, w, _ = image.shape\n",
    "                x, y = int(wrist.x * w), int(wrist.y * h)\n",
    "                cv2.putText(image, f'{hand_label} Hand', (x - 50, y - 20),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        vector = extract_feature_vector(results)\n",
    "        cv2.rectangle(image, (0, 0), (600, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, f'Feature Vector Shape: {vector.shape}', (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('Real-Time Feature Extraction', image)\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6ed6208-9be5-4940-91cb-e373e48800ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words found: 2000\n",
      "First 20 words: ['book', 'drink', 'computer', 'before', 'chair', 'go', 'clothes', 'who', 'candy', 'cousin', 'deaf', 'fine', 'help', 'no', 'thin', 'walk', 'year', 'yes', 'all', 'black']\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "def get_all_sign_words(json_path):\n",
    "    sign_words = []\n",
    "    seen = set()\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        for entry in data:\n",
    "            word = entry['gloss']\n",
    "            if word not in seen:\n",
    "                sign_words.append(word)\n",
    "                seen.add(word)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {json_path} file not found.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Failed to decode JSON file at {json_path}.\")\n",
    "        return []\n",
    "    return sign_words\n",
    "if __name__ == '__main__':\n",
    "    WLASL_PATH = \"C:\\\\Gautam\\\\Projects\\\\sign_language_translator\\\\model_development\\\\WLASL_v0.3.json\"\n",
    "    all_words = get_all_sign_words(WLASL_PATH)\n",
    "    if all_words:\n",
    "        print(f\"Total unique words found: {len(all_words)}\")\n",
    "        print(\"First 20 words:\", all_words[:20])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df7529a-a879-4a26-b902-e968dd930540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
